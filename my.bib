@inproceedings{kumar2017,
    author = {Kumar, T. K. Satish and Xu, Hong and Tang, Zheng and Kumar, Anoop and Rogers, Craig Milo and Knoblock, Craig A.},
    year = {2017},
    booktitle = {Proceedings of the 29th IEEE International Conference on Tools with Artificial Intelligence (ICTAI)},
    title = {A Distributed Logical Filter for Connected Row Convex Constraints},
    url = {http://files.hong.me/papers/kumar2017.pdf},
}
@inproceedings{kumar2018,
    author = {Kumar, T. K. Satish and Xu, Hong and Tang, Zheng and Kumar, Anoop and Rogers, Craig Milo and Knoblock, Craig A.},
    title = {Alert Generation in Execution Monitoring Using Resource Envelopes},
    booktitle = {Proceedings of the 31st International FLAIRS Conference (FLAIRS)},
    year = {2018},
    url = {http://files.hong.me/papers/kumar2018.pdf}
}
@inproceedings{N19-eidosdelphi,
    title = "Eidos, INDRA, \& Delphi: From Free Text to Executable Causal Models",
    author = {
        Sharp, Rebecca and
  Pyarelal,Adarsh and
  Gyori,Benjamin M. and
  Alcock,Keith and
  Laparra,Egoitz and
  Valenzuela-Esc{\'a}rcega,Marco A. and
  Nagesh,Ajay and
  Yadav,Vikas and
  Bachman,John A. and
  Tang,Zheng and
  Lent,Heather and
  Luo,Fan and
  Paul,Mithun and
  Bethard,Steven and
  Barnard,Kobus and
  Morrison,Clayton and
  Surdeanu, Mihai},
    booktitle = "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = {https://aclanthology.org/N19-4008/}
}
@inproceedings{zheng-tang-2019-edin,
    title = "Exploring Interpretability in Event Extraction: Multitask Learning of a Neural Event Classifier and an Explanation Decoder",
    author = "Tang, Zheng and Hahn-Powell, Gustave and Surdeanu, Mihai",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop",
    month = jul,
    year = "2020",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-srw.23/"
}
@inproceedings{zheng-tang-2021-edin,
    title = "Interpretability Rules: Jointly Bootstrapping a Neural Relation Extractor with an Explanation Decoder",
    author = "Tang, Zheng and Surdeanu, Mihai",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: TrustNLP Workshop",
    year = "2021",
    url = "https://aclanthology.org/2021.trustnlp-1.1.pdf"
}
@inproceedings{van-etal-2021-may-help,
    title = "How May {I} Help You? Using Neural Text Simplification to Improve Downstream {NLP} Tasks",
    author = "Van, Hoang  and
      Tang, Zheng  and
      Surdeanu, Mihai",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.343",
    pages = "4074--4080",
    abstract = "The general goal of text simplification (TS) is to reduce text complexity for human consumption. In this paper, we investigate another potential use of neural TS: assisting machines performing natural language processing (NLP) tasks. We evaluate the use of neural TS in two ways: simplifying input texts at prediction time and augmenting data to provide machines with additional information during training. We demonstrate that the latter scenario provides positive effects on machine performance on two separate datasets. In particular, the latter use of TS improves the performances of LSTM (1.82{--}1.98{\%}) and SpanBERT (0.7{--}1.3{\%}) extractors on TACRED, a complex, large-scale, real-world relation extraction task. Further, the same setting yields improvements of up to 0.65{\%} matched and 0.62{\%} mismatched accuracies for a BERT text classifier on MNLI, a practical natural language inference dataset.",
}
@article{10.1162/coli_a_00463,
    author = {Tang, Zheng and Surdeanu, Mihai},
    title = "{It Takes Two Flints to Make a Fire: Multitask Learning of Neural Relation and Explanation Classifiers}",
    journal = {Computational Linguistics},
    volume = {49},
    number = {1},
    pages = {117-156},
    year = {2023},
    month = {03},
    abstract = "{We propose an explainable approach for relation extraction that mitigates the tension between generalization and explainability by jointly training for the two goals. Our approach uses a multi-task learning architecture, which jointly trains a classifier for relation extraction, and a sequence model that labels words in the context of the relations that explain the decisions of the relation classifier. We also convert the model outputs to rules to bring global explanations to this approach. This sequence model is trained using a hybrid strategy: supervised, when supervision from pre-existing patterns is available, and semi-supervised otherwise. In the latter situation, we treat the sequence model’s labels as latent variables, and learn the best assignment that maximizes the performance of the relation classifier. We evaluate the proposed approach on the two datasets and show that the sequence model provides labels that serve as accurate explanations for the relation classifier’s decisions, and, importantly, that the joint training generally improves the performance of the relation classifier. We also evaluate the performance of the generated rules and show that the new rules are a great add-on to the manual rules and bring the rule-based system much closer to the neural models.}",
    issn = {0891-2017},
    doi = {10.1162/coli_a_00463},
    url = {https://doi.org/10.1162/coli\_a\_00463},
    eprint = {https://direct.mit.edu/coli/article-pdf/49/1/117/2068962/coli\_a\_00463.pdf},
}
@inproceedings{surdeanu-etal-2022-taxonomy,
    title = "Taxonomy Builder: a Data-driven and User-centric Tool for Streamlining Taxonomy Construction",
    author = "Surdeanu, Mihai  and
      Hungerford, John  and
      Chan, Yee Seng  and
      MacBride, Jessica  and
      Gyori, Benjamin  and
      Zupon, Andrew  and
      Tang, Zheng  and
      Qiu, Haoling  and
      Min, Bonan  and
      Zverev, Yan  and
      Hilverman, Caitlin  and
      Thomas, Max  and
      Andrews, Walter  and
      Alcock, Keith  and
      Zhang, Zeyu  and
      Reynolds, Michael  and
      Bethard, Steven  and
      Sharp, Rebecca  and
      Laparra, Egoitz",
    booktitle = "Proceedings of the Second Workshop on Bridging Human--Computer Interaction and Natural Language Processing",
    month = jul,
    year = "2022",
    address = "Seattle, Washington",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.hcinlp-1.1",
    pages = "1--10",
}
@inproceedings{tang-surdeanu-2023-bootstrapping,
    title = "Bootstrapping Neural Relation and Explanation Classifiers",
    author = "Tang, Zheng  and
      Surdeanu, Mihai",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-short.5",
    pages = "48--56",
    abstract = "We introduce a method that self trains (or bootstraps) neural relation and explanation classifiers. Our work expands the supervised approach of CITATION, which jointly trains a relation classifier with an explanation classifier that identifies context words important for the relation at hand, to semi-supervised scenarios. In particular, our approach iteratively converts the explainable models{'} outputs to rules and applies them to unlabeled text to produce new annotations.Our evaluation on the TACRED dataset shows that our method outperforms the rule-based model we started from by 15 F1 points, outperforms traditional self-training that relies just on the relation classifier by 5 F1 points, and performs comparatively with the prompt-based approach of CITATION (without requiring an additional natural language inference component).",
}
@inproceedings{
chen2024alpagasus,
title={Alpagasus: Training a Better Alpaca Model with Fewer Data},
author={Lichang Chen and Shiyang Li and Jun Yan and Hai Wang and Kalpa Gunaratna and Vikas Yadav and Zheng Tang and Vijay Srinivasan and Tianyi Zhou and Heng Huang and Hongxia Jin},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=FdVXgSJhvz}
}
@inproceedings{
li2024instructionfollowing,
title={Instruction-following Evaluation through Verbalizer Manipulation},
author={Shiyang Li and Jun Yan and Hai Wang and Zheng Tang and Xiang Ren and Vijay Srinivasan and Hongxia Jin},
booktitle={2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
year={2024},
url={https://openreview.net/forum?id=sCeQOBtPmp}
}
@inproceedings{
yan2024backdooring,
title={Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection},
author={Jun Yan and Vikas Yadav and Shiyang Li and Lichang Chen and Zheng Tang and Hai Wang and Vijay Srinivasan and Xiang Ren and Hongxia Jin},
booktitle={2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
year={2024},
url={https://openreview.net/forum?id=ucBWb6LKUH}
}
@inproceedings{10.1145/3626772.3657959,
author = {Yadav, Vikas and Tang, Zheng and Srinivasan, Vijay},
title = {PAG-LLM: Paraphrase and Aggregate with Large Language Models for Minimizing Intent Classification Errors},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657959},
doi = {10.1145/3626772.3657959},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2569–2573},
numpages = {5},
keywords = {aggregation, generation confidence, intent classification, large langauge model, paraphrasing},
location = {Washington DC, USA},
series = {SIGIR '24}
}
@article{gao2025disp,
  title={Disp-llm: Dimension-independent structural pruning for large language models},
  author={Gao, Shangqian and Lin, Chi-Heng and Hua, Ting and Tang, Zheng and Shen, Yilin and Jin, Hongxia and Hsu, Yen-Chang},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={72219--72244},
  year={2025}
}
